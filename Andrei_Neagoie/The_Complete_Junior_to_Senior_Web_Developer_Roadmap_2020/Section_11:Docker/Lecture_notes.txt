Bruno's Request
Hey there Bruno.
People are getting frustrated with the local development.
And that is slowing us down.
We would really apricate if could use Docker instead.
Docker has changed the entire landscape.

Section Overview
One problem we applications getting bigger and bigger.
It becomes one big monster and make things hard to manage.
Certain Operating System.
Most often when we are running on different computers.
We must be able to run it anywhere.
Companies like Netflix and Amazon have their products composed.
Monolithic App and we have microservices instead.
Breaking down our App into smaller services.
Each service may have their own requierments it can be a challenge to run on different services.
Think about our previous section.
How difficult it was to setting up things.
We had to do many things.
Would it be nice to just to setup things.
Docker take care so everything runs everywhere.
Docker helps us to take care of this.

Docker Containers
History lesson.
Why was this tool built.
Before we had docker we had virtual machines.
Has a entire operating system.
Like to have a computer on another computer.
We had big things it took a few minutes to boot up.
Docker came along and wrapped up the system.
Anything you could install on a server we could use it for docker.
Container is so small and thin.
Container is much faster to run small single applications.
They are just easy to grow and grow.
Container orchestration and is managed by kubernetes.
Make sure that it will run everywhere.
Can be explained by a diagram.
Host
On top of the host is the containers.
Something we create with Docker.
Within an container we have an image.
Within the Image we can have an node server.
The enviroment is isolated from the whole server.
Image has an writeable file-system.
Docker executes the image that is in the container.
The image thing sounds complicated.
We have Docker Hub is kind like NPM.
We can download images.
I have different node versions on the docker hub.
Using the image and it is going to create the container.
Development setup we are going to create an docker-container.
Bring in our Postgres.
We can do it in just an single command.
Modern companies has load balancer the distribute that to different API-servers that are just docker-containers.
API connected to different databases.
Having Docker helps us a lot.

Installing Docker
We have our documentation.
Docker for mac and for windows.

Dockerfile
Let's go to the SmartBrain-API.
What we need to do.
Build an image-file.
touch Dockerfile.
This Dockerfile will be our image.
In there we can include images.
We check the node-version.

In the Docker file below.

Tells with image we want to run from the Docker Hub.
FROM node:carbon

Commands we want to run in the container CMD.

CMD ["/bin/bash"] 

Run the command.
docker build -t superawesomecontainer .
It runs faster the second time.
It cache the image.
docker run -it superawesomecontainer
We are inside the container.
Go into the bash-shell.
You always want to check the container.
Without the it we can go into the container.

Docker Commands
We had docker run and docker build.
It was to go into the Container.
We just have our node-version.
We extend the container to have the server.
Be nice to run in the background.
docker run -it -d superawesomecontainer.
docker ps we see all the containers that are currently running.
docker exec -it hash bash.
Let's us to get back to the container.
docker stop containerid.
Now we stopped it.

Dockerfile 2
We learned an new few docker commands.
How can we run the smart-brain in the container.
WORKDIR /usr/src/smart-brain-api;
It is the working directory.
When we go into the bash we will be in the WORKDIR.
COPY ./ ./
RUN npm install We want to run in this in the container.
What is the difference between RUN and CMD.
Run is what is called image built-step.
CMD is executing by default when you are executing the image.
Docker run-it superawesomecontainer.
ls list all the packages now.
Container dosen't know about the machine.
We actually have to tell the container to expose. 
docker run -it -p 3000:3000 superawesomecontainer.

Docker-Compose
We learned how docker is created.
We can use it for our Postgres.
Orchesters the development.
Bruno's request that we are running from on command without for us to set up a database.
We are gonna use Docker Compose that contains
-Redis
-Database
-API
To combine database we create an docker-compose.yml file.
version: '3.6'

services:
   smart-brain-api:
      container_name: backend 
      #image: node:8.11.1
      build: ./
      command: npm start
      working_dir: /app
      ports:
        - "3000:3000"

The way we run is docker-compose build.
Our Docker Compose to create an service called smart-brain-api.
docker-compose run smart-brain-api.
Docker-compose run smart-brain-api.
npm install morgan.
const morgan = require('morgan').
docker-compose down getting down.
Bring down containers.
docker-compose up --build.
Bring up the containers.
Docker-Compose builds our image instead of the container.

Docker-Compose 2 
Using the build.
It maps the containers port to our port.
Docker-compose run.
Docker-compose run is not the most optional.
Docker-compose up is more ideal to start several containers.
Any time we add something to the we rebuild everything and rerun everything.
We can add volumes:
             - ./:/usr/src/smart-brain-api

Container file-system is going to see the changes.
Now it is listen to changes.
Volumes is complex topic.

Docker-Compose 3
Quick commands.
docker-compose down
Bring down all the containers.
Run docker-compose up.
docker-compose exec smart-brain-api bash.
Then i'am in the bash of the container.

Docker-Compose 4
Make sure that there is no containers that run.

To add PostgresSql it is quite simple.
We just add an new service in the docker-compose.yml
postgres:
     container_name: postgres
     image: postgres
     ports:
       - "5432:5432"
Cannot create container Postgresql in use.
The name is already set.
A more likely scenario it is a username and a password.
We are going to use enviromental variable.
POSTGRES_USER: Sally
POSTGRES_PASSWORD: secret
POSTGRES_DB: smart-brain-docker
POSTGRES_HOST: postgres
links:
   -postgres

Now we have all the enviromental variables.
host : process.env.POSTGRES_HOST, 
user : process.env.POSTGRES_USER,
password : process.env.POSTGRES_PASSWORD,
database : process.env.POSTGRES_DB 

Now we have to restart the docker compose.
Roll Sally dosen't exists.
Thinking about it critically.
We need to give the enviromental variables to the Postgre too.
POSTGRES_URI: postgres://sally:secret@postgres:5432/smart-brain-docker
Go to the server file.js and remove and this to the connection.
connection: process.env.POSTGRES_URI.

Docker-Compose 5
Go to the Front-End and start with 
npm-start.
Create an new user.
We are getting  an error now.
Looking on the login.
We need to create tables.

Docker-Compose 6
Reason is that keep developers dosen't have to create new tables with Docker.
Following along.
Created an PostgreFolder.
We are gonna customize the docker.
FROM postgres:10.3

ADD /table/ /docker-entrypoint-initdb.d/tables/
We want create our user and login tables.
We create Table-folders.
Good practice when you work with SQL is that you use transaction.
If something fails don't create the user table.

BEGIN TRANSACTION;

COMMIT.

We are creating two SQL files.
deploy_schemas.sql.
\i gonna to run the new file we create.
\i /docker-entrypoint-initdb.d/tables/users.sql
\i /docker-entrypoint-initdb.d/tables/login.sql
Run these SQL commands.
Now we have the tables.
Create Database call.
Anytime we wanna use an docker-file we want to build ./postgres.

Docker-Compose 7
Wouldn't be nice that there are some users we wanna test with.
\i /docker-entrypoint-initdb.d/seed/seed.sql
Create an new folder called Seed.
BEGIN TRANSACTION

Insert into users(name, email, entries, joined) values('Jessie','jessie@gmail.com',)
COMMIT

Section Summary

